{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pet project where I will be trying to build a chatbot using a transformer architecture based neural network. I'll be using the famous [Cornell Movie Dialog Dataset](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) to train my model. Lets see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some pretty important dependencies\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameys\\.keras\\datasets\\cornell movie-dialogs corpus\n"
     ]
    }
   ],
   "source": [
    "# Need to download the dataset and save it offile for the first time\n",
    "# Keras utils have very useful functions and get_file is one of them.\n",
    "# Function brings the file and stores in cache(temp) directory.\n",
    "zip_path = tf.keras.utils.get_file(fname='cornell_movie_dialogs.zip',\n",
    "    origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "# Follow thedirectory structure to the input files\n",
    "dataset_path = os.path.join(\n",
    "    os.path.dirname(zip_path), \"cornell movie-dialogs corpus\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameys\\.keras\\datasets\\cornell movie-dialogs corpus\\movie_lines.txt \n",
      "C:\\Users\\ameys\\.keras\\datasets\\cornell movie-dialogs corpus\\movie_conversations.txt\n"
     ]
    }
   ],
   "source": [
    "movie_lines_path = os.path.join(dataset_path, 'movie_lines.txt')\n",
    "movie_conversations_path = os.path.join(dataset_path, \"movie_conversations.txt\")\n",
    "print(movie_lines_path, \"\\n\"+movie_conversations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below are the steps that I am going to follow to preprocess the data**\n",
    "\n",
    "    -Process each input text and remove all the special characters except for .,!\n",
    "    \n",
    "    -Build a tokenizer \n",
    "    \n",
    "    -Tokenize each input and also add a START and END token\n",
    "    \n",
    "    -Choose a MAX_LENGTH and pad/strip the inputs accordingly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES = 100000\n",
    "def process_text(text):\n",
    "    text = text.lower().strip()\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.,!]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_data():\n",
    "    id2line = {}\n",
    "    with open(movie_lines_path, errors='ignore') as fp:\n",
    "        for line in fp.readlines():\n",
    "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "            id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(movie_conversations_path, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "            # get conversation in a list of line ID\n",
    "            conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "            for i in range(len(conversation) - 1):\n",
    "                inputs.append(process_text(id2line[conversation[i]]))\n",
    "                outputs.append(process_text(id2line[conversation[i + 1]]))\n",
    "            if len(inputs) > MAX_SAMPLES:\n",
    "                return inputs, outputs\n",
    "            \n",
    "questions, answers = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: i should not even be listening to you , since my council said no . but santangel tells me you are a man of honor and sincerity . . . and sanchez , that you are not a fool .\n",
      "Sample answer: no more than the woman who said she would take granada from the moors .\n"
     ]
    }
   ],
   "source": [
    "print('Sample question: {}'.format(questions[595]))\n",
    "print('Sample answer: {}'.format(answers[595]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer with the vocabulary in both questions and answers\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(questions + answers, \n",
    "                                                                    target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN:  [39.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting Maximum Sequence Length\n",
    "input_lengths = [len(text.split()) for text in questions+answers]\n",
    "plt.hist(input_lengths, bins = 100, histtype='step', label='Sequence Length Distribution')\n",
    "plt.axvline(np.quantile(input_lengths, [0.95]))\n",
    "plt.legend();\n",
    "print(\"MEAN: \",np.quantile(input_lengths, [0.95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we set the sequence length to 40 words then we can consider 95% of data without stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sentence length\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "\n",
    "# Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # tokenize sentence\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "        \n",
    "    # pad tokenized sentences\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs,\n",
    "                                                                     maxlen=MAX_LENGTH,\n",
    "                                                                     truncating='post',\n",
    "                                                                     padding='post',\n",
    "                                                                    )\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs,\n",
    "                                                                      maxlen=MAX_LENGTH,\n",
    "                                                                      padding='post',\n",
    "                                                                      truncating='post',\n",
    "                                                                     )\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8212\n",
      "Number of samples: 100003\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
    "print('Number of samples: {}'.format(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ({inputs: (None, 40), dec_inputs: (None, 39)}, {outputs: (None, 39)}), types: ({inputs: tf.int32, dec_inputs: tf.int32}, {outputs: tf.int32})>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 30000\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# removing the END_TOKEN from answers\n",
    "# removing START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Scaled dot product Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scaled dot product attention mechanism has 3 inputs Query(Q), Key(K) and Value(V).\n",
    "\n",
    "Among these 3 inputs firstly, Q and K^T undergo a dot product\n",
    "\n",
    "the result of dot product is scaled by dividing with sqrt(dimensions)\n",
    "\n",
    "the scaled result is them passed through a softmax function which yields the attension weights \n",
    "for each word with rest of the words in that input sequence.\n",
    "\n",
    "At the end the attention weights undergo dot product with the V\n",
    "\"\"\"\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
