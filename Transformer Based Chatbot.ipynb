{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pet project where I will be trying to build a chatbot using a transformer architecture based neural network. I'll be using the famous [Cornell Movie Dialog Dataset](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) to train my model. Lets see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some pretty important dependencies\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameys\\.keras\\datasets\\cornell movie-dialogs corpus\n"
     ]
    }
   ],
   "source": [
    "# Need to download the dataset and save it offile for the first time\n",
    "# Keras utils have very useful functions and get_file is one of them.\n",
    "# Function brings the file and stores in cache(temp) directory.\n",
    "zip_path = tf.keras.utils.get_file(fname='cornell_movie_dialogs.zip',\n",
    "    origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "# Follow thedirectory structure to the input files\n",
    "dataset_path = os.path.join(\n",
    "    os.path.dirname(zip_path), \"cornell movie-dialogs corpus\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameys\\.keras\\datasets\\cornell movie-dialogs corpus\\movie_lines.txt \n",
      "C:\\Users\\ameys\\.keras\\datasets\\cornell movie-dialogs corpus\\movie_conversations.txt\n"
     ]
    }
   ],
   "source": [
    "movie_lines_path = os.path.join(dataset_path, 'movie_lines.txt')\n",
    "movie_conversations_path = os.path.join(dataset_path, \"movie_conversations.txt\")\n",
    "print(movie_lines_path, \"\\n\"+movie_conversations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below are the steps that I am going to follow to preprocess the data**\n",
    "\n",
    "    -Process each input text and remove all the special characters except for .,!\n",
    "    \n",
    "    -Build a tokenizer \n",
    "    \n",
    "    -Tokenize each input and also add a START and END token\n",
    "    \n",
    "    -Choose a MAX_LENGTH and pad/strip the inputs accordingly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES = 100000\n",
    "def process_text(text):\n",
    "    text = text.lower().strip()\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.,!]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_data():\n",
    "    id2line = {}\n",
    "    with open(movie_lines_path, errors='ignore') as fp:\n",
    "        for line in fp.readlines():\n",
    "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "            id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(movie_conversations_path, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "            # get conversation in a list of line ID\n",
    "            conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "            for i in range(len(conversation) - 1):\n",
    "                inputs.append(process_text(id2line[conversation[i]]))\n",
    "                outputs.append(process_text(id2line[conversation[i + 1]]))\n",
    "            if len(inputs) > MAX_SAMPLES:\n",
    "                return inputs, outputs\n",
    "            \n",
    "questions, answers = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: i should not even be listening to you , since my council said no . but santangel tells me you are a man of honor and sincerity . . . and sanchez , that you are not a fool .\n",
      "Sample answer: no more than the woman who said she would take granada from the moors .\n"
     ]
    }
   ],
   "source": [
    "print('Sample question: {}'.format(questions[595]))\n",
    "print('Sample answer: {}'.format(answers[595]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer with the vocabulary in both questions and answers\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(questions + answers, \n",
    "                                                                    target_vocab_size=2**15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Maximum Sequence Length\n",
    "input_lengths = [len(text.split()) for text in questions+answers]\n",
    "plt.hist(input_lengths, bins = 100, histtype='step', label='Sequence Length Distribution')\n",
    "plt.axvline(np.quantile(input_lengths, [0.95]))\n",
    "plt.legend();\n",
    "print(\"MEAN: \",np.quantile(input_lengths, [0.95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we set the sequence length to 40 words then we can consider 95% of data without stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sentence length\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "\n",
    "# Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # tokenize sentence\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "        \n",
    "    # pad tokenized sentences\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs,\n",
    "                                                                     maxlen=MAX_LENGTH,\n",
    "                                                                     truncating='post',\n",
    "                                                                     padding='post',\n",
    "                                                                    )\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs,\n",
    "                                                                      maxlen=MAX_LENGTH,\n",
    "                                                                      padding='post',\n",
    "                                                                      truncating='post',\n",
    "                                                                     )\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
    "print('Number of samples: {}'.format(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 30000\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# removing the END_TOKEN from answers\n",
    "# removing START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Scaled dot product Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scaled dot product attention mechanism has 3 inputs Query(Q), Key(K) and Value(V).\n",
    "\n",
    "Among these 3 inputs firstly, Q and K^T undergo a dot product\n",
    "\n",
    "the result of dot product is scaled by dividing with sqrt(dimensions)\n",
    "\n",
    "the scaled result is them passed through a softmax function which yields the attension weights \n",
    "for each word with rest of the words in that input sequence.\n",
    "\n",
    "At the end the attention weights undergo dot product with the V\n",
    "\"\"\"\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a MultiHead Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, scale_d, num_heads, name=\"Multi Head Attention Layer\"):\n",
    "        super(MultiHeadAttentionLayer, self).__init__(name=name)\n",
    "        self.scale_d = scale_d\n",
    "        self.num_heads = num_heads\n",
    "        # scale_d should e a multiple of num_heads\n",
    "        assert (self.scale_d % self.num_heads == 0)\n",
    "        self.depth = self.scale_d // self.num_heads\n",
    "        \n",
    "        self.query_dense = tf.keras.layers.Dense(units = self.scale_d)\n",
    "        self.key_dense = tf.keras.layers.Dense(units = self.scale_d)\n",
    "        self.value_dense = tf.keras.layers.Dense(units = self.scale_d)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units = self.scale_d)\n",
    "    \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        # understand what this method does and why we are reshaping and transposing the input matrix\n",
    "        # Find out out the shape of the input before and after this method call\n",
    "        inputs = tf.reshape(inputs, \n",
    "                            shape = (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm = [0,2,1,3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "        \n",
    "        # converting to split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        # embedding attention mechanism\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0,2,1,3])\n",
    "        \n",
    "        # concating all the heads\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      shape=(batch_size, -1, self.scale_d))\n",
    "        \n",
    "        # Final Dense layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "Since the multihead attention layer is now ready, I'll use it to build the transformer.\n",
    "\n",
    "But before passing any input to a transformer we need to mask certain token in our input. We do this masking so that the model doesn't treat those tokens as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During preprocessing I have padded the sequences that were of length less than 40, \n",
    "# now I have to mask those padded tokens\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I'll write another method that will help me in masking the future tokens in the sequence.\n",
    "# So that to predict the third word onlyh the first and second words will be used\n",
    "def create_lookahead_mask(x):\n",
    "    seq_len = tf.shape(x)[-1]\n",
    "    lookahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(padding_mask, lookahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_lookahead_mask(tf.constant([[1,2,0,4,5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "Since a transformer doesn't consist of convolution operation like CNN and recurrence mechanism like RNN/LSTM it needs a new way to understand the relation between tokens in a sequence. Positional encoding does this job, it consists of the relative information of tokens in a sequence.\n",
    "\n",
    "Positional encoding vector is added to the embedding vector. Embeddings represent a word in a d-dimensional space in such a way that words with similar meaning are closer to each other. But the information about how close the words in a sentence is not present in embeddings, by adding the positional encoding to those embeddings, words will be closer to each other based on the <i>similarity of their meaning</i> and <i>their position in the sentence</i> both.\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d_{scale}}}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, scale_d, name = 'PositionalEncoding'):\n",
    "        super(PositionalEncoding, self).__init__(name='PositionalEncoding')\n",
    "        self.positional_encoding = self.positional_ecnoding(position, scale_d)\n",
    "    \n",
    "    def get_angles(self, position, i, scale_d):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(scale_d, tf.float32))\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_ecnoding(self, position, scale_d):\n",
    "        angle_radians = self.get_angles(position = tf.range(position, dtype = tf.float32)[:, tf.newaxis], \n",
    "                                        i = tf.range(scale_d, dtype=tf.float32)[tf.newaxis, :], \n",
    "                                        scale_d = scale_d)\n",
    "        # apply sin over the even indices\n",
    "        sines = tf.math.sin(angle_radians[:, 0::2])\n",
    "        # apply cos over the odd indices\n",
    "        cosines = tf.math.cos(angle_radians[:, 1::2])\n",
    "        \n",
    "        positional_encoding = tf.concat([sines, cosines], axis = -1)\n",
    "        positional_encoding = positional_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(positional_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.positional_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.positional_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "All the bare metal code is ready now I will create an encoder layer and its work will be to consume a sequence as input and give us encoded representation as output.\n",
    "\n",
    "The encoder layer will have 2 steps,\n",
    "1) MultiHeadAttention Layer\n",
    "2) 2 Dense layers followed by a dropout layer\n",
    "\n",
    "Each of the sub layers above have residual connections around them which are followed by layer normalization. Yes it is not BatchNormalization. Layer normalization and weight normalization are [different](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/).\n",
    "\n",
    "The residual connections play the same role that they play in CNNs. The skip connections inside them help us to train deeper networks by not letting out model fall into the pits of vanishing gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, scale_d, num_heads, dropout, name = \"Encoder\"):\n",
    "    inputs = tf.keras.Input(shape = (None, scale_d), name = 'Input')\n",
    "    # Also create a tensor for MASK\n",
    "    padding_mask = tf.keras.Input(shape=(1,1,None), name = \"MASK\")\n",
    "    \n",
    "    attention = MultiHeadAttentionLayer(scale_d=scale_d, \n",
    "                                        num_heads=num_heads, \n",
    "                                        name='attention')(\n",
    "        {'query' : inputs, \n",
    "         'key' : inputs, \n",
    "         'value' : inputs, \n",
    "         'mask' : padding_mask})\n",
    "    attention = tf.keras.layers.Dropout(dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    output = tf.keras.layers.Dense(units=scale_d)(output)\n",
    "    \n",
    "    output = tf.keras.layers.Dropout(dropout)(output)\n",
    "    output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + output)\n",
    "    \n",
    "    return tf.keras.Model(inputs = [inputs, padding_mask], outputs = output, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = encoder_layer(units=512, \n",
    "                       scale_d=128, \n",
    "                       num_heads=4, \n",
    "                       dropout=0.3, \n",
    "                       name='Encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(text_encoder, show_shapes=True)\n",
    "# ,to_file='encoder_layer.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "use the encoder layer above to create an encoder. It should take the concatenated vector of sequences passed through an embedding layer and positional encodings as inputs.\n",
    "\n",
    "1) Input Embedding\n",
    "\n",
    "2) Positional Encoding\n",
    "\n",
    "3) # of Encoder layers\n",
    "\n",
    "\n",
    "The input embeddings are passed through an embedding layer as then their positional encoding vector is also calculated. Both of them are cancatenated and added passed as input to Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, \n",
    "            num_layers, \n",
    "            units, \n",
    "            scale_d, \n",
    "            num_heads, \n",
    "            dropout, \n",
    "            name = 'Encoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name = \"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='MASK')\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, scale_d)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(scale_d, dtype=tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, scale_d)(embeddings)\n",
    "    \n",
    "    output = tf.keras.layers.Dropout(dropout)(embeddings)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        output = encoder_layer(units=units, \n",
    "                                scale_d=scale_d, \n",
    "                                num_heads=num_heads, \n",
    "                                dropout=dropout, \n",
    "                                name=\"EncoderLayer{}\".format(i))([output, padding_mask])\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=output, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = encoder(vocab_size=8000,\n",
    "                         units=512,\n",
    "                         num_layers=2,\n",
    "                         scale_d=128,\n",
    "                         num_heads=4,\n",
    "                         dropout=0.3,\n",
    "                         name='sample_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(sample_encoder, show_shapes=True)\n",
    "# ,to_file=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "Now it is tome to create a Decode layer. It will consist of,\n",
    "\n",
    "1) Masked Multi-head attention layer (With both look ahead and Padding mask)\n",
    "\n",
    "2) Multi head attention layer (With padding mask). Key and Value receive the encoder outputs as inputs and Query will receive the out from Masked Multi-head attention layer\n",
    "\n",
    "3) 2 Dense layers followed by Dropout\n",
    "\n",
    "\n",
    "Each of these sublayers has a residual connection around them which is followed by Layer Normalization. \n",
    "\n",
    "Query receives the decoder's attention layer output as input and key receives the encoder's output as input. The attention scores act as the weights or represent the importance of decoders input(nothing but the encoders output). In other words the decoder predicts the next word by looking at the encoder's output and self-attending its own output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(units, scale_d, num_heads, dropout, name='Decoder Layer'):\n",
    "    inputs = tf.keras.Input(shape=(None, scale_d), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, scale_d), name=\"encoder_input\")\n",
    "    \n",
    "    look_ahead_mask = tf.keras.Input(shape=(1,None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1,1,None), name = \"padding_mask\")\n",
    "    \n",
    "    attention1 = MultiHeadAttentionLayer(scale_d, \n",
    "                                         num_heads, \n",
    "                                         name=\"attention1\")(inputs = {\n",
    "        'query' : inputs,\n",
    "        'key' : inputs,\n",
    "        'value' : inputs,\n",
    "        'mask' : look_ahead_mask\n",
    "    })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "    \n",
    "    attention2 = MultiHeadAttentionLayer(scale_d, \n",
    "                                         num_heads, \n",
    "                                         name=\"Attention2\")(inputs = {\n",
    "        'query' : attention1,\n",
    "        'key' : enc_outputs,\n",
    "        'value' : enc_outputs,\n",
    "        'mask' : padding_mask\n",
    "    })\n",
    "    \n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(units = units, activation = 'relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units = scale_d)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], \n",
    "                                 outputs=outputs,\n",
    "                                 name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder = decoder_layer(units=512, \n",
    "                               scale_d=128, \n",
    "                               num_heads=4, \n",
    "                               dropout=0.3, \n",
    "                               name = 'decoder_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(sample_decoder, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Decode will consist of,\n",
    "\n",
    "1) Output Embeddings\n",
    "\n",
    "2) Positional Encoding\n",
    "\n",
    "3) N decoder layers\n",
    "\n",
    "The target is passed through an embedding layer which is then summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, \n",
    "            num_layers, \n",
    "            units, \n",
    "            scale_d, \n",
    "            num_heads, \n",
    "            dropout, \n",
    "            name = \"Decoder\"):\n",
    "    inputs = tf.keras.Input(shape = (None,), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape = (None,scale_d), name = \"encoder_output\")\n",
    "    \n",
    "    look_ahead_mask = tf.keras.Input(shape=(1,None,None), name = \"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1,1,None), name = \"padding_mask\")\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, scale_d)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(scale_d, dtype=tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, scale_d)(embeddings)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(units=units, \n",
    "                                scale_d=scale_d, \n",
    "                                num_heads=num_heads, \n",
    "                                dropout=dropout, \n",
    "                                name = \"decoder_layer_{}\".format(i)\n",
    "                               )(inputs = [outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], \n",
    "                                 outputs=outputs, \n",
    "                                 name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder = decoder(vocab_size=8000, \n",
    "                        units=512,\n",
    "                        scale_d=128,\n",
    "                        num_layers=2, \n",
    "                        num_heads=4,\n",
    "                        dropout=0.3,\n",
    "                        name=\"Decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(sample_decoder, show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "Transformer is a combination of encoder, decoder and a final layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, \n",
    "                num_layers, \n",
    "                units, \n",
    "                scale_d, \n",
    "                num_heads, \n",
    "                dropout, \n",
    "                name = \"Transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "    \n",
    "    enc_padding_mask = tf.keras.layers.Lambda( create_padding_mask, \n",
    "                                             output_shape=(1,1,None), \n",
    "                                             name = \"enc_padding_mask\")(inputs)\n",
    "    # Now mask the future tokens in he decoder input at first attention layer\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(create_lookahead_mask, \n",
    "                                             output_shape=(1, None, None), \n",
    "                                             name=\"look_ahead_mask\")(dec_inputs)\n",
    "    # mask the encoder outputs for second attention layer\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask, \n",
    "                                              output_shape=(1,1,None), \n",
    "                                              name='dec_padding_mask')(inputs)\n",
    "    \n",
    "    enc_outputs = encoder(vocab_size=vocab_size, \n",
    "                        num_layers=num_layers,\n",
    "                        units=units,\n",
    "                        scale_d=scale_d, \n",
    "                        num_heads=num_heads,\n",
    "                        dropout=dropout)(inputs=[inputs, enc_padding_mask])\n",
    "    dec_outputs = decoder(vocab_size=vocab_size,\n",
    "                         num_layers=num_layers,\n",
    "                         units=units,\n",
    "                         scale_d=scale_d,\n",
    "                         num_heads=num_heads,\n",
    "                         dropout=dropout)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "    outputs = tf.keras.layers.Dense(units = vocab_size, name='outputs')(dec_outputs)\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer = transformer(vocab_size=8000,\n",
    "                                num_layers=2,\n",
    "                                units = 512,\n",
    "                                scale_d=128,\n",
    "                                num_heads=4,\n",
    "                                dropout=0.3,\n",
    "                                name=\"Sample Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(sample_transformer, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "\n",
    "Model Initialization\n",
    "\n",
    "See the [paper](https://arxiv.org/pdf/1706.03762.pdf) for all the other versions of the transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "NUM_LAYERS=2\n",
    "NUM_HEADS=8\n",
    "UNITS=512\n",
    "SCALE_D=256\n",
    "DROPOUT=0.1\n",
    "\n",
    "model = transformer(vocab_size=VOCAB_SIZE, \n",
    "                    num_layers=NUM_LAYERS, \n",
    "                    units=UNITS, \n",
    "                    scale_d=SCALE_D, \n",
    "                    num_heads=NUM_HEADS, \n",
    "                    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS Function\n",
    "\n",
    "Since the target sequences are masked we also need to modify the loss function to adjust for those paddings and doesn't consider them for loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                        reduction='none')(y_true=y_true, y_pred=y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Learning Rate\n",
    "\n",
    "Use the Adam optimizer with a custom [LR Scheduler](https://arxiv.org/pdf/1706.03762.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLRScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, scale_d, warmup_steps = 800):\n",
    "        super().__init__()\n",
    "        self.scale_d = scale_d\n",
    "        self.scale_d = tf.cast(self.scale_d, dtype=tf.float32)\n",
    "        \n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        a = tf.math.rsqrt(step)\n",
    "        b = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.scale_d)*tf.math.minimum(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_learning_rate = CustomLRScheduler(scale_d=128, warmup_steps=200)\n",
    "plt.plot(sample_learning_rate(tf.range(20000, dtype=tf.float32)))\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"LearningRate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomLRScheduler(scale_d=SCALE_D, warmup_steps=800)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH-1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred=y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "\n",
    "Now it is time to train the Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCS = 50\n",
    "\n",
    "model.fit(dataset, epochs=EPOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
